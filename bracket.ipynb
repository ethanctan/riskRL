{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game, Map\n",
    "from itertools import product, permutations\n",
    "import numpy as np\n",
    "import random\n",
    "from consts import TROOP_LIMIT, NUM_PLAYERS, DISCOUNT\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, game_state: dict, agent_id: int): \n",
    "\n",
    "        # Initialize game info\n",
    "        self.agent_id = agent_id\n",
    "        self.game_counter = 0\n",
    "        self.game_log = [[game_state]] #list of lists, each list of states corresponds to the state of one game\n",
    "        self.nodes = game_state[\"nodes\"]\n",
    "        self.nPlayers = len(game_state[\"owners\"])\n",
    "        self.edges = game_state[\"edges\"]\n",
    "\n",
    "\n",
    "        # Initialize log for self actions\n",
    "        self.actions_log = [[]]\n",
    "\n",
    "    def initialize_actions(self):\n",
    "        # each action is a tuple (start_node, end_node)\n",
    "        actions = []\n",
    "        for i in range(len(self.nodes)):\n",
    "            for j in range(len(self.nodes)):\n",
    "                if i != j:\n",
    "                    actions.append((i, j))\n",
    "        return actions\n",
    "    \n",
    "    def get_neighbors(self, node: int):\n",
    "        neighbors = [node]\n",
    "        for edge in self.edges:\n",
    "            if edge[0] == node:\n",
    "                neighbors.append(edge[1])\n",
    "            elif edge[1] == node:\n",
    "                neighbors.append(edge[0])\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def initialize_R(self):\n",
    "        print(f\"agent {self.agent_id} initializing R\")\n",
    "\n",
    "        TROOP_KILL_REWARD_MULTIPLIER = 1\n",
    "        TERRITORY_GAIN_REWARD = 10\n",
    "        TERRITORY_REINFORCE_REWARD_MULTIPLIER = 1\n",
    "        ENEMY_KILL_REWARD = 100\n",
    "\n",
    "        # the reward matrix assigns rewards to each state-action pair\n",
    "        R = defaultdict(dict)\n",
    "\n",
    "        for state in self.states:\n",
    "            for node1, node2 in permutations(state, 2):\n",
    "                (start_node, start_node_owner, start_node_troops) = node1\n",
    "                (end_node, end_node_owner, end_node_troops) = node2\n",
    "                action = (start_node, end_node)\n",
    "\n",
    "                # Check conditions only if start_node_owner is the agent\n",
    "                if start_node_owner == self.agent_id and end_node_owner != self.agent_id:\n",
    "                    owners = [node[1] for node in state]\n",
    "                    if owners.count(end_node_owner) == 1:\n",
    "                        # Action kills an enemy\n",
    "                        R[state][action] = ENEMY_KILL_REWARD\n",
    "                    elif start_node_troops >= end_node_troops:\n",
    "                        # Action would kill an enemy territory if successful\n",
    "                        R[state][action] = TERRITORY_GAIN_REWARD\n",
    "                    else:\n",
    "                        # Action reduces the number of troops in an enemy territory\n",
    "                        R[state][action] = (end_node_troops - start_node_troops) * TROOP_KILL_REWARD_MULTIPLIER\n",
    "                elif start_node_owner == self.agent_id and end_node_owner == self.agent_id:\n",
    "                    # if an action reinforces a node adjacent to an enemy, then the reward is the number of troops added\n",
    "                    neighbors = self.get_neighbors(end_node)\n",
    "                    for neighbor in neighbors:\n",
    "                        for node in state:\n",
    "                            if node[0] == neighbor and node[1] != self.agent_id:\n",
    "                                R[state][action] = start_node_troops * TERRITORY_REINFORCE_REWARD_MULTIPLIER\n",
    "                                break\n",
    "                        if R[state].get(action) is not None:\n",
    "                            break\n",
    "                        else:\n",
    "                            R[state][action] = 0\n",
    "                                \n",
    "                else: # set reward to 0 for invalid actions\n",
    "                    R[state][action] = 0\n",
    "\n",
    "        return R\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_P(self): #P gives transition probabilities from state to state given action\n",
    "        # each state is a frozenset of tuples where each tuple is (node, owner, troops)\n",
    "        # each action is a tuple (start_node, end_node)\n",
    "        print(f\"agent {self.agent_id} initializing P\")\n",
    "        default_prob = 1 / len(self.states)\n",
    "        P = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: default_prob)))\n",
    "\n",
    "        return P\n",
    "\n",
    "    def initialize_random_pi(self):\n",
    "        # each action is a tuple (start_node, end_node)\n",
    "        # given each state, select a random start node and a random end node\n",
    "        print(f\"agent {self.agent_id} initializing pi\")\n",
    "        pi = {}\n",
    "        for state in self.states:\n",
    "            # pick a random valid action\n",
    "            valid_actions = []\n",
    "            nodes_owned_by_agent = []\n",
    "\n",
    "            for node in state:\n",
    "                if node[1] == self.agent_id:\n",
    "                    nodes_owned_by_agent.append(node[0])\n",
    "\n",
    "            for action in self.actions:\n",
    "                if action[0] in nodes_owned_by_agent:\n",
    "                    valid_actions.append(action)\n",
    "\n",
    "            if len(valid_actions) == 0:\n",
    "                pi[state] = random.choice(self.actions)\n",
    "                continue\n",
    "                    \n",
    "            pi[state] = random.choice(valid_actions)\n",
    "\n",
    "        return pi\n",
    "\n",
    "    def initialize_states(self):\n",
    "        # each state will be labelled with a unique frozenset of tuples where each tuple is (node, owner, troops)\n",
    "        print(f\"agent {self.agent_id} initializing states\")\n",
    "        node_ids = range(len(self.nodes))\n",
    "        owners = range(NUM_PLAYERS + 1)  # Including a 'no owner' state\n",
    "        troops = range(TROOP_LIMIT + 1)\n",
    "\n",
    "        print(f\"agent {self.agent_id} generating all possible node states\")\n",
    "\n",
    "        # Generate all possible states for a single node\n",
    "        single_node_states = list(product(owners, troops))\n",
    "        print(len(single_node_states))\n",
    "\n",
    "        print(f\"agent {self.agent_id} generating all possible game states\")\n",
    "\n",
    "        # Generate all possible game states\n",
    "        all_game_states = product(*[single_node_states for _ in node_ids])\n",
    "\n",
    "        # Create a dictionary with game states as frozensets\n",
    "        states = {}\n",
    "        i = 0\n",
    "        for state in all_game_states:\n",
    "            if i % 100000 == 0:\n",
    "                print(f\"agent {self.agent_id} handling game state {i}\")\n",
    "            i += 1\n",
    "            game_state = frozenset((node_id,) + state[i] for i, node_id in enumerate(node_ids))\n",
    "            states[game_state] = None  # The value can be anything, e.g., game state score\n",
    "\n",
    "        return states\n",
    "\n",
    "    def initialize_new_game(self, game_state: dict):\n",
    "        self.game_counter += 1\n",
    "        self.game_log.append([game_state]) #initialize new game state log\n",
    "        self.actions_log.append([]) #initialize new actions log\n",
    "\n",
    "    def update_current_game_state(self, game_state: dict):\n",
    "        self.game_log[self.game_counter].append(game_state)\n",
    "    \n",
    "    def turn_game_state_into_frozenset(self, game_state: dict):\n",
    "        # want to make a frozenset of tuples (node_index, owner, troops)\n",
    "        state_frozenset = []\n",
    "        for node_index, (node, owner) in enumerate(zip(game_state[\"nodes\"], game_state[\"owners\"])):\n",
    "            state_frozenset.append((node_index, owner, node))\n",
    "        return frozenset(state_frozenset)\n",
    "\n",
    "    def make_move(self):\n",
    "        current_game_state = self.game_log[self.game_counter][-1]\n",
    "        current_game_state_frozenset = self.turn_game_state_into_frozenset(current_game_state)\n",
    "        action = self.pi[current_game_state_frozenset]\n",
    "        self.actions_log[self.game_counter].append(action)\n",
    "        return action\n",
    "            \n",
    "    def approximate_P(self):\n",
    "        print(f\"agent {self.agent_id} approximating P\")\n",
    "\n",
    "        for game_state, action, next_game_state in zip(self.game_log[self.game_counter], self.actions_log[self.game_counter], self.game_log[self.game_counter][1:]):\n",
    "            game_state_frozenset = self.turn_game_state_into_frozenset(game_state)\n",
    "            next_game_state_frozenset = self.turn_game_state_into_frozenset(next_game_state)\n",
    "            self.P[game_state_frozenset][action][next_game_state_frozenset] += 1 \n",
    "            # set all other values in self.P[game_state_frozenset][action] to 0\n",
    "            for next_state in self.states:\n",
    "                if next_state != next_game_state_frozenset:\n",
    "                    self.P[game_state_frozenset][action][next_state] = 0\n",
    "\n",
    "        # normalize P \n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                total = sum(self.P[state][action].values())\n",
    "                if total:\n",
    "                    for next_state in self.states:\n",
    "                        self.P[state][action][next_state] /= total\n",
    "\n",
    "        print(\"P approximated\")\n",
    "        \n",
    "\n",
    "    def update_pi(self, pi: dict):  \n",
    "        self.pi = pi\n",
    "\n",
    "class GreedyAgent(Agent):\n",
    "    def __init__(self, game_state: dict, agent_id: int):\n",
    "        super().__init__(game_state, agent_id)\n",
    "    \n",
    "\n",
    "    def initialize_R(self):\n",
    "        print(f\"agent {self.agent_id} initializing R\")\n",
    "\n",
    "        TROOP_KILL_REWARD_MULTIPLIER = 10 # x10 of original\n",
    "        TERRITORY_GAIN_REWARD = 100 # x10 of original\n",
    "        TERRITORY_REINFORCE_REWARD_MULTIPLIER = 0 # x0 of original\n",
    "        ENEMY_KILL_REWARD = 100\n",
    "\n",
    "        # the reward matrix assigns rewards to each state-action pair\n",
    "        R = defaultdict(dict)\n",
    "\n",
    "        for state in self.states:\n",
    "            for node1, node2 in permutations(state, 2):\n",
    "                (start_node, start_node_owner, start_node_troops) = node1\n",
    "                (end_node, end_node_owner, end_node_troops) = node2\n",
    "                action = (start_node, end_node)\n",
    "\n",
    "                # Check conditions only if start_node_owner is the agent\n",
    "                if start_node_owner == self.agent_id and end_node_owner != self.agent_id:\n",
    "                    owners = [node[1] for node in state]\n",
    "                    if owners.count(end_node_owner) == 1:\n",
    "                        # Action kills an enemy\n",
    "                        R[state][action] = ENEMY_KILL_REWARD\n",
    "                    elif start_node_troops >= end_node_troops:\n",
    "                        # Action would kill an enemy territory if successful\n",
    "                        R[state][action] = TERRITORY_GAIN_REWARD\n",
    "                    else:\n",
    "                        # Action reduces the number of troops in an enemy territory\n",
    "                        R[state][action] = (end_node_troops - start_node_troops) * TROOP_KILL_REWARD_MULTIPLIER\n",
    "                elif start_node_owner == self.agent_id and end_node_owner == self.agent_id:\n",
    "                    # if an action reinforces a node adjacent to an enemy, then the reward is the number of troops added\n",
    "                    neighbors = self.get_neighbors(end_node)\n",
    "                    for neighbor in neighbors:\n",
    "                        for node in state:\n",
    "                            if node[0] == neighbor and node[1] != self.agent_id:\n",
    "                                R[state][action] = start_node_troops * TERRITORY_REINFORCE_REWARD_MULTIPLIER\n",
    "                                break\n",
    "                        if R[state].get(action) is not None:\n",
    "                            break\n",
    "                        else:\n",
    "                            R[state][action] = 0\n",
    "                                \n",
    "                else: # set reward to 0 for invalid actions\n",
    "                    R[state][action] = 0\n",
    "\n",
    "        return R\n",
    "\n",
    "class DefensiveAgent(Agent):\n",
    "    def __init__(self, game_state: dict, agent_id: int):\n",
    "        super().__init__(game_state, agent_id)\n",
    "    \n",
    "\n",
    "    def initialize_R(self):\n",
    "        print(f\"agent {self.agent_id} initializing R\")\n",
    "\n",
    "        TROOP_KILL_REWARD_MULTIPLIER = 1\n",
    "        TERRITORY_GAIN_REWARD = 10\n",
    "        TERRITORY_REINFORCE_REWARD_MULTIPLIER = 20 # x20 of original\n",
    "        ENEMY_KILL_REWARD = 100\n",
    "\n",
    "        # the reward matrix assigns rewards to each state-action pair\n",
    "        R = defaultdict(dict)\n",
    "\n",
    "        for state in self.states:\n",
    "            for node1, node2 in permutations(state, 2):\n",
    "                (start_node, start_node_owner, start_node_troops) = node1\n",
    "                (end_node, end_node_owner, end_node_troops) = node2\n",
    "                action = (start_node, end_node)\n",
    "\n",
    "                # Check conditions only if start_node_owner is the agent\n",
    "                if start_node_owner == self.agent_id and end_node_owner != self.agent_id:\n",
    "                    owners = [node[1] for node in state]\n",
    "                    if owners.count(end_node_owner) == 1:\n",
    "                        # Action kills an enemy\n",
    "                        R[state][action] = ENEMY_KILL_REWARD\n",
    "                    elif start_node_troops >= end_node_troops:\n",
    "                        # Action would kill an enemy territory if successful\n",
    "                        R[state][action] = TERRITORY_GAIN_REWARD\n",
    "                    else:\n",
    "                        # Action reduces the number of troops in an enemy territory\n",
    "                        R[state][action] = (end_node_troops - start_node_troops) * TROOP_KILL_REWARD_MULTIPLIER\n",
    "                elif start_node_owner == self.agent_id and end_node_owner == self.agent_id:\n",
    "                    # if an action reinforces a node adjacent to an enemy, then the reward is the number of troops added\n",
    "                    neighbors = self.get_neighbors(end_node)\n",
    "                    for neighbor in neighbors:\n",
    "                        for node in state:\n",
    "                            if node[0] == neighbor and node[1] != self.agent_id:\n",
    "                                R[state][action] = start_node_troops * TERRITORY_REINFORCE_REWARD_MULTIPLIER\n",
    "                                break\n",
    "                        if R[state].get(action) is not None:\n",
    "                            break\n",
    "                        else:\n",
    "                            R[state][action] = 0\n",
    "                                \n",
    "                else: # set reward to 0 for invalid actions\n",
    "                    R[state][action] = 0\n",
    "\n",
    "        return R\n",
    "\n",
    "class TerritorialEmphasisAgent(Agent):\n",
    "    def __init__(self, game_state: dict, agent_id: int):\n",
    "        super().__init__(game_state, agent_id)\n",
    "    \n",
    "\n",
    "    def initialize_R(self):\n",
    "        print(f\"agent {self.agent_id} initializing R\")\n",
    "\n",
    "        TROOP_KILL_REWARD_MULTIPLIER = 1\n",
    "        TERRITORY_GAIN_REWARD = 50 # x5 of original\n",
    "        TERRITORY_REINFORCE_REWARD_MULTIPLIER = 1 \n",
    "        ENEMY_KILL_REWARD = 100\n",
    "\n",
    "        # the reward matrix assigns rewards to each state-action pair\n",
    "        R = defaultdict(dict)\n",
    "\n",
    "        for state in self.states:\n",
    "            for node1, node2 in permutations(state, 2):\n",
    "                (start_node, start_node_owner, start_node_troops) = node1\n",
    "                (end_node, end_node_owner, end_node_troops) = node2\n",
    "                action = (start_node, end_node)\n",
    "\n",
    "                # Check conditions only if start_node_owner is the agent\n",
    "                if start_node_owner == self.agent_id and end_node_owner != self.agent_id:\n",
    "                    owners = [node[1] for node in state]\n",
    "                    if owners.count(end_node_owner) == 1:\n",
    "                        # Action kills an enemy\n",
    "                        R[state][action] = ENEMY_KILL_REWARD\n",
    "                    elif start_node_troops >= end_node_troops:\n",
    "                        # Action would kill an enemy territory if successful\n",
    "                        R[state][action] = TERRITORY_GAIN_REWARD\n",
    "                    else:\n",
    "                        # Action reduces the number of troops in an enemy territory\n",
    "                        R[state][action] = (end_node_troops - start_node_troops) * TROOP_KILL_REWARD_MULTIPLIER\n",
    "                elif start_node_owner == self.agent_id and end_node_owner == self.agent_id:\n",
    "                    # if an action reinforces a node adjacent to an enemy, then the reward is the number of troops added\n",
    "                    neighbors = self.get_neighbors(end_node)\n",
    "                    for neighbor in neighbors:\n",
    "                        for node in state:\n",
    "                            if node[0] == neighbor and node[1] != self.agent_id:\n",
    "                                R[state][action] = start_node_troops * TERRITORY_REINFORCE_REWARD_MULTIPLIER\n",
    "                                break\n",
    "                        if R[state].get(action) is not None:\n",
    "                            break\n",
    "                        else:\n",
    "                            R[state][action] = 0\n",
    "                                \n",
    "                else: # set reward to 0 for invalid actions\n",
    "                    R[state][action] = 0\n",
    "\n",
    "        return R\n",
    "    \n",
    "class SoldierEmphasisAgent(Agent):\n",
    "    def __init__(self, game_state: dict, agent_id: int):\n",
    "        super().__init__(game_state, agent_id)\n",
    "    \n",
    "\n",
    "    def initialize_R(self):\n",
    "        print(f\"agent {self.agent_id} initializing R\")\n",
    "\n",
    "        TROOP_KILL_REWARD_MULTIPLIER = 5 # x5 of original\n",
    "        TERRITORY_GAIN_REWARD = 10\n",
    "        TERRITORY_REINFORCE_REWARD_MULTIPLIER = 1 \n",
    "        ENEMY_KILL_REWARD = 100\n",
    "\n",
    "        # the reward matrix assigns rewards to each state-action pair\n",
    "        R = defaultdict(dict)\n",
    "\n",
    "        for state in self.states:\n",
    "            for node1, node2 in permutations(state, 2):\n",
    "                (start_node, start_node_owner, start_node_troops) = node1\n",
    "                (end_node, end_node_owner, end_node_troops) = node2\n",
    "                action = (start_node, end_node)\n",
    "\n",
    "                # Check conditions only if start_node_owner is the agent\n",
    "                if start_node_owner == self.agent_id and end_node_owner != self.agent_id:\n",
    "                    owners = [node[1] for node in state]\n",
    "                    if owners.count(end_node_owner) == 1:\n",
    "                        # Action kills an enemy\n",
    "                        R[state][action] = ENEMY_KILL_REWARD\n",
    "                    elif start_node_troops >= end_node_troops:\n",
    "                        # Action would kill an enemy territory if successful\n",
    "                        R[state][action] = TERRITORY_GAIN_REWARD\n",
    "                    else:\n",
    "                        # Action reduces the number of troops in an enemy territory\n",
    "                        R[state][action] = (end_node_troops - start_node_troops) * TROOP_KILL_REWARD_MULTIPLIER\n",
    "                elif start_node_owner == self.agent_id and end_node_owner == self.agent_id:\n",
    "                    # if an action reinforces a node adjacent to an enemy, then the reward is the number of troops added\n",
    "                    neighbors = self.get_neighbors(end_node)\n",
    "                    for neighbor in neighbors:\n",
    "                        for node in state:\n",
    "                            if node[0] == neighbor and node[1] != self.agent_id:\n",
    "                                R[state][action] = start_node_troops * TERRITORY_REINFORCE_REWARD_MULTIPLIER\n",
    "                                break\n",
    "                        if R[state].get(action) is not None:\n",
    "                            break\n",
    "                        else:\n",
    "                            R[state][action] = 0\n",
    "                                \n",
    "                else: # set reward to 0 for invalid actions\n",
    "                    R[state][action] = 0\n",
    "\n",
    "        return R\n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, game_state: dict, agent_id: int):\n",
    "        super().__init__(game_state, agent_id)\n",
    "\n",
    "    def make_move(self):\n",
    "        num_nodes = len(self.nodes)\n",
    "        start_node = random.randint(0, num_nodes - 1)\n",
    "        end_node = random.randint(0, num_nodes - 1)\n",
    "        return (start_node, end_node)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC PROGRAMMING\n",
    "\n",
    "class DynamicProgramming:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def computeQfromV(self, V):\n",
    "        print(\"Computing Q from V\")\n",
    "        Q = defaultdict(lambda: defaultdict(float))\n",
    "        for state in self.agent.states:\n",
    "            for a in self.agent.actions:\n",
    "                expected_value = self.agent.R[state][a]\n",
    "                for next_state, probability in self.agent.P[state][a].items():\n",
    "                    expected_value += probability * V.get(next_state, 0)\n",
    "                Q[state][a] = expected_value\n",
    "        return Q\n",
    "\n",
    "    def extractMaxPiFromV(self, V):\n",
    "        print(\"Extracting max pi from V\")\n",
    "        pi = defaultdict(lambda: None)\n",
    "        Q = self.computeQfromV(V)\n",
    "        for state in Q:\n",
    "            pi[state] = max(Q[state], key=Q[state].get)\n",
    "        return pi\n",
    "    \n",
    "\n",
    "    def approxPolicyEvaluation(self, pi, tolerance=0.0001):\n",
    "        print(\"Approximating policy evaluation\")\n",
    "        epsilon = float('inf')\n",
    "        V = defaultdict(float)\n",
    "\n",
    "        i = 0\n",
    "        while epsilon > tolerance:\n",
    "            print(f\"Approximating policy evaluation iteration {i}\")\n",
    "            nextV = V.copy()\n",
    "            for s in self.agent.states:\n",
    "                Rpis = self.agent.R[s][pi[s]]\n",
    "                Ppis = self.agent.P[s][pi[s]]  # defaultdict\n",
    "                expected_value = 0\n",
    "                \n",
    "                for next_state, probability in Ppis.items():\n",
    "                    expected_value += (probability * V[next_state])\n",
    "                \n",
    "                nextV[s] = Rpis + expected_value * DISCOUNT\n",
    "\n",
    "            epsilon = max(abs(nextV[s] - V[s]) for s in self.agent.states)\n",
    "            print(f\"epsilon: {epsilon}\")\n",
    "            # print(f\"V: {V}\")\n",
    "            V = nextV.copy()\n",
    "            i += 1\n",
    "        return V\n",
    "\n",
    "    def policyIterationStep(self, pi):\n",
    "        return self.extractMaxPiFromV(self.approxPolicyEvaluation(pi))\n",
    "\n",
    "    def policyIteration(self, initial_pi):\n",
    "        pi = initial_pi.copy()\n",
    "        iteration_count = 0\n",
    "\n",
    "        while True:\n",
    "            next_pi = self.policyIterationStep(pi)\n",
    "            iteration_count += 1\n",
    "            print(f\"Policy iteration step {iteration_count}\")\n",
    "\n",
    "            if pi == next_pi:\n",
    "                break\n",
    "            pi = next_pi\n",
    "\n",
    "        V = self.approxPolicyEvaluation(pi, 0.1)\n",
    "        return pi, V, iteration_count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define game\n",
    "\n",
    "numNodes = 5\n",
    "edges = [(0, 2), (2, 3), (3, 1), (1, 0), (0, 4), (2, 4), (1, 4)]\n",
    "defaultWeight = 1\n",
    "reinforceAmount = 1\n",
    "reinforcePlayersOnly = True\n",
    "killTurn = 100\n",
    "\n",
    "initialPosition = {\n",
    "    1: [1],\n",
    "    2: [2]\n",
    "}\n",
    "\n",
    "map = Map(numNodes, edges, defaultWeight)\n",
    "game = Game(map, initialPosition, reinforceAmount, reinforcePlayersOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent 1 initializing states\n",
      "agent 1 generating all possible node states\n",
      "12\n",
      "agent 1 generating all possible game states\n",
      "agent 1 handling game state 0\n",
      "agent 1 handling game state 100000\n",
      "agent 1 handling game state 200000\n",
      "agent 1 initializing pi\n",
      "agent 1 initializing P\n",
      "agent 1 initializing R\n",
      "agent 1 initializing R\n",
      "agent 1 initializing R\n",
      "agent 1 initializing R\n",
      "agent 1 initializing R\n"
     ]
    }
   ],
   "source": [
    "# Define agents\n",
    "default_agent = Agent(game.getState(), 1)\n",
    "greedy_agent = GreedyAgent(game.getState(), 1)\n",
    "defensive_agent = DefensiveAgent(game.getState(), 1)\n",
    "territorial_emphasis_agent = TerritorialEmphasisAgent(game.getState(), 1)\n",
    "soldier_emphasis_agent = SoldierEmphasisAgent(game.getState(), 1)\n",
    "random_agent = RandomAgent(game.getState(), 2)\n",
    "\n",
    "agents = [default_agent, greedy_agent, defensive_agent, territorial_emphasis_agent, soldier_emphasis_agent, random_agent]\n",
    "\n",
    "default_agent.states = default_agent.initialize_states()\n",
    "default_agent.actions = default_agent.initialize_actions()\n",
    "default_agent.pi = default_agent.initialize_random_pi()\n",
    "default_agent.P = default_agent.initialize_P()\n",
    "default_agent.R = default_agent.initialize_R()\n",
    "\n",
    "for agent in agents[1:5]:\n",
    "    agent.states = default_agent.states\n",
    "    agent.actions = default_agent.actions\n",
    "    agent.pi = default_agent.pi\n",
    "    agent.P = default_agent.P\n",
    "    agent.R = agent.initialize_R()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent <__main__.Agent object at 0x1053c5950>\n",
      "Game number 0, turn number 0\n",
      "agent 1 approximating P\n",
      "P approximated\n",
      "Approximating policy evaluation\n",
      "Approximating policy evaluation iteration 0\n",
      "epsilon: 100.0\n",
      "Approximating policy evaluation iteration 1\n",
      "epsilon: 0.0\n",
      "Extracting max pi from V\n",
      "Computing Q from V\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m agent\u001b[39m.\u001b[39mapproximate_P()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m agent_dp \u001b[39m=\u001b[39m DynamicProgramming(agent)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pi, V, _ \u001b[39m=\u001b[39m agent_dp\u001b[39m.\u001b[39mpolicyIterationStep(agent\u001b[39m.\u001b[39mpi)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m agent\u001b[39m.\u001b[39mupdate_pi(pi)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ectan/Coding-new/riskRL_test/riskRL/bracket.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# initialize new game and pass to agents\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Train agents against random\n",
    "ITERS = 10\n",
    "\n",
    "for agent_number, agent in enumerate(agents[0:5]):\n",
    "    print(f\"Training agent {agent_number}\")\n",
    "    for game_number in range(ITERS):\n",
    "        # Play a training game\n",
    "        for turn_number in range(killTurn):\n",
    "            print(f\"Game number {game_number}, turn number {turn_number}\")\n",
    "\n",
    "            # Get moves\n",
    "            agent_move = agent.make_move()\n",
    "            random_move = random_agent.make_move()\n",
    "            moves = [(1, agent_move[0], agent_move[1]), (2, random_move[0], random_move[1])]\n",
    "\n",
    "            # update game\n",
    "            if game.turn(moves):\n",
    "                break\n",
    "\n",
    "            # Update agent\n",
    "            game_state = game.getState()\n",
    "            agent.update_current_game_state(game_state)\n",
    "            print(game_state)\n",
    "\n",
    "        # approximate P\n",
    "        agent.approximate_P()\n",
    "\n",
    "        agent_dp = DynamicProgramming(agent)\n",
    "        pi, V, _ = agent_dp.policyIteration(agent.pi)\n",
    "        agent.update_pi(pi)\n",
    "\n",
    "        # initialize new game and pass to agents\n",
    "        game = Game(map, initialPosition, reinforceAmount, reinforcePlayersOnly)\n",
    "        agent.initialize_new_game(game.getState())\n",
    "\n",
    "    print(f\"Training done for agent {agent_number}\")\n",
    "print(\"Training done\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
